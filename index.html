<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>MimicSound</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-EDF010G6PN"></script>
  <script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-EDF010G6PN');
  </script>

  <script type="module" src="https://unpkg.com/@google/model-viewer/dist/model-viewer.min.js"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
  <script type="text/javascript" src="https://code.jquery.com/jquery-1.11.0.min.js"></script>
  <script type="text/javascript" src="https://code.jquery.com/jquery-migrate-1.2.1.min.js"></script>
  <script src="https://unpkg.com/interactjs/dist/interact.min.js"></script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" type="text/css" href="./static/slick/slick.css"/>
  <link rel="stylesheet" type="text/css" href="./static/slick/slick-theme.css"/>
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://github.com/zcswdt">
            DRaMa
          </a>
        </div>
      </div>
    </div>
  </div>
</nav> -->

</body>
</html>



<style>
  .author-block {
    display: flex;
    align-items: center; /* ËÆ©ÂÜÖÂÆπÂûÇÁõ¥Â±Ö‰∏≠ */
  }

  .author-block sup {
    font-size: 0.8em; /* Áº©Â∞è * Âè∑ */
    margin-left: 2px; /* Ê∑ªÂä†‰∏ÄÁÇπÂ∑¶‰æßÈó¥Ë∑ù */
    vertical-align: top; /* ËÆ© * Âè∑Èù†È°∂ÈÉ®ÂØπÈΩê */
  }
</style>

<section class="hero">
  <div class="hero-body">
    <div class="container">
      <div class="container has-text-centered">
        <h1 class="title is-1 publication-title">
          MimicSound: Learning Bimanual Manipulation  <span style="display: block;"> from Audio-Visual Human Videos </span>
          <!-- TactileAloha: <span style="display: block;">Learning to Manipulate with Tactile Sensing</span> -->
      </h1>
        <div class="is-size-5 publication-authors" style="display: flex; justify-content: center; gap: 30px;">
          <div class="author-block">
            <a href="https://github.com/guningquan">Ningquan Gu</a>
          </div>
          <div class="author-block">
            <a href="https://www.eee.hku.hk/people/kazuhiro-kosuge/"> Kazuhiro Kosuge</a>   
        </div>
          <div class="author-block">
            <a href="https://neuro.mech.tohoku.ac.jp/">Mitsuhiro Hayashibe</a><sup>*</sup>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>




<div class="column has-text-centered">
  <div class="publication-links">

    
    <span class="link-block">
      <a href=""
         class="external-link button is-normal is-rounded is-dark" target="_blank">
        <span class="icon">
            <i class="fas fa-file-pdf"></i>
        </span>
        <span>Paper (coming Soon)</span>
      </a>
    </span>


    <!-- ML Code Link -->
    <span class="link-block">
      <a href="https://github.com/guningquan/mimicsound-ml"
         class="external-link button is-normal is-rounded is-dark">
        <span class="icon">
          <i class="fab fa-github"></i>
        </span>
        <span>Code</span>
      </a>
    </span>
  </div>
</div>


      </div>
    </div>
  </div>
</section>






<section class="section">
  <div class="container is-max-desktop">

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Learning robot manipulation from human visual videos has shown promise, but leveraging audio information in human data remains underexplored.
            To address it, we propose MimicSound, a unified End2End audio-visual imitation learning framework that novelly co-trains on human audio-visual data together with robot demonstrations using a dual-branch design.
            Human-robot data collection employs a shared top-view camera and microphone to ensure spatial and auditory alignment, with human audio augmented by robot noise to reduce cross-domain gaps.
            To achieve temporal consistency, we introduce Timestep-aligned Audio Synchronization for per-step multimodal alignment in robot data, and Marker-based Human-Robot Demonstration Alignment for phase-wise speed alignment between the two data sources across long-horizon tasks.
            During training, MimicSound adopts a shared audio-visual encoder: the vision module encodes top-view robot images or hand-masked human images to mitigate domain bias, while the shared audio encoder, based on a pretrained Audio Spectrogram Transformer (AST) fine-tuned via Weight-Decomposed Low-Rank Adaptation (DoRA), extracts unified auditory representations for both branches.
            Two Bidirectional Cross-modal Attention Modules then dynamically fuse audio and visual features in each branch before feeding them into the Action Chunking with Transformers (ACT) policy.
            We evaluate MimicSound on three bimanual audio-visual tasks, demonstrating the advantage of learning from human audio-visual demonstrations, while ablations show the contribution of each component.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>

<hr/>


<div style="margin-bottom: 40px;"></div>



<section class="hero teaser">
  <div class="hero-body" style="background: linear-gradient(135deg, #A8E6CF, #56C596); padding: 20px 0;">
    <div class="container is-max-desktop" style="text-align: center;">
      <h2 class="title is-3" style="color: #FFFFFF; margin: 0; font-weight: bold;">Approach Overview</h2>
    </div>
  </div>

<div class="container is-max-desktop" style="margin-top: 20px;">
  <div style="text-align: center;">
    <img src="./static/images/overview.jpg" alt="Approach Overview" 
         style="max-width: 90%; height: auto; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
  </div>
  <p style="margin-top: 12px; font-size: 0.95em; color: #444; line-height: 1.6; text-align: justify;">
Fig. 1. Overview of the MimicSound.
		The framework adopts a dual-branch design for human and robot data. 
		In the robot branch, multi-view RGB images $o_{R,t}^{cams}$ serve as visual inputs: 
		the left and right arm views are encoded by ResNet models pre-trained on ImageNet, 
		while the top-view image and audio $o_{R,t}^{aud}$ are processed by the shared audio-visual encoder. 
		Robot joint states $o_{R,t}^{jnt}$ are normalized and encoded via a linear layer. 
		The human branch takes the top-view RGB image $o_{H,t}^{cam}$, robot-noise-augmented audio $o_{H,t}^{aud}$, 
		and normalized hand positions $o_{H,t}^{pos}$, also encoded by a linear layer. 
		The shared encoder includes a vision and an audio module: 
		top-view robot image or hand-masked human image is encoded by a shared ResNet, 
		while audio is downsampled, converted into Mel spectrograms, and encoded by an AST 
		initialized from AudioSet. 
		During training, the AST backbone is frozen and only DoRA modules are fine-tuned. 
		In each branch, audio and visual features are fused by their Bidirectional Cross-modal Attention module before being fed into the ACT-based policy.
		The policy predicts action chunks for robot joints and end-effector positions in the robot branch, 
		and for hand positions in the human branch, where the latter shares the same linear layer as the robot end-effector prediction. 
		Losses are computed separately for each branch (robot joints, robot end positions, and human end positions) 
		and jointly optimized through a shared gradient update during training.
  </p>
</div>
</section>

<div style="margin-bottom: 40px;"></div>
<hr/>
<section class="hero teaser">
  <div class="hero-body" style="background: linear-gradient(135deg, #56CCF2, #2F80ED); padding: 20px 0;">
    <div class="container is-max-desktop" style="text-align: center;">
      <h2 class="title is-3" style="color: #FFFFFF; margin: 0;"> Human-Robot Audio-Visual Data Collection and  <br> Autonomous Bimanual Skill Learning with Audio.</h2>
      <p style="color: #FFFFFF; margin-top: 10px; font-size: 1.1em;">üîä Please turn on your speaker for full experience</p>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div style="height: 20px;"></div> <!-- Èó¥Èöî -->

      <div style="display: flex; flex-wrap: wrap; justify-content: center;">
        <div style="display: flex; justify-content: center; align-items: center; flex-direction: column; margin-bottom: 20px;">
          <video autoplay controls muted loop playsinline 
                 style="width: 100%; max-width: 90%; height: auto; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
            <source src="./static/videos/mimicsound-demo.mp4" type="video/mp4">
          </video>
        </div>
      </div>

    </div>
  </div>
</section>


<section class="footer">
  <div class="container content is-max-desktop" style="padding: 20px;">
    <div style="text-align: left;">
      If you have any questions, please feel free to contact 
      <a href="gu.ningquan.t1@dc.tohoku.ac.jp">gu.ningquan.t1@dc.tohoku.ac.jp</a>.
    </div>
  </div>
</section>

<script type="text/javascript" src="./static/slick/slick.min.js"></script>
</body>
</html>
